{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier  \n",
    "\n",
    "data_path = '/legodata/zhaoj/cvd_risk_time2/data_1_8/processed'\n",
    "# def get_cohort(data_path, fram, cmd_arg):\n",
    "#     if fram:\n",
    "#         if cmd_arg.impute_0:\n",
    "#             df_cohort = pd.read_csv(os.path.join(data_path, 'df_fram_cohort_0.csv'))\n",
    "#         else:\n",
    "#             df_cohort = pd.read_csv(os.path.join(data_path, 'df_fram_cohort.csv'))\n",
    "#     else:\n",
    "#         if cmd_arg.impute_0:\n",
    "#             df_cohort = pd.read_csv(os.path.join(data_path, 'df_full_cohort_0.csv'))\n",
    "#             df_cohort = df_cohort.drop(['predict'], axis=1)\n",
    "#         else:\n",
    "#             df_cohort = pd.read_csv(os.path.join(data_path, 'df_full_cohort.csv'))\n",
    "#             df_cohort = df_cohort.drop(['predict'], axis=1)\n",
    "\n",
    "#     if cmd_arg.phemed_bin:\n",
    "#         df_cohort = convert_phemed_to_bin(df_cohort)\n",
    "\n",
    "#     if not cmd_arg.demo:\n",
    "#         print('drop_demo')\n",
    "#         df_cohort = df_cohort.drop(['AGE_AT_BASELINE', 'RACE_W', 'RACE_B', 'RACE_U', 'GENDER'], axis=1)\n",
    "\n",
    "#     # if cmd_arg.norm:\n",
    "#     #     df_cohort = norm_feat(df_cohort)\n",
    "\n",
    "#     if cmd_arg.reg:\n",
    "#         # df_cohort = l1_regularization(df=df_cohort,\n",
    "#         #                               alpha_inv=100)\n",
    "#         df_cohort = xgb_regularization(df_cohort=df_cohort)\n",
    "\n",
    "#     return df_cohort\n",
    "\n",
    "def convert_phemed_to_bin(df_cohort):\n",
    "    print('CONVERTING PHEMED TO BIN')\n",
    "    col_lst = list(df_cohort.columns)\n",
    "\n",
    "    med_col_lst = [col for col in col_lst if 'med_' in col]\n",
    "    phe_col_lst = [col for col in col_lst if 'phe_' in col]\n",
    "\n",
    "    df_med = df_cohort[med_col_lst]\n",
    "    df_phe = df_cohort[phe_col_lst]\n",
    "\n",
    "    for col in med_col_lst:\n",
    "        df_med[col] = df_med[col].map(lambda x: 'yes' if x != 0 else 'no')\n",
    "    df_cohort[med_col_lst] = pd.get_dummies(df_med, drop_first=True)\n",
    "\n",
    "    for col in phe_col_lst:\n",
    "        df_phe[col] = df_phe[col].map(lambda x: 'yes' if x != 0 else 'no')\n",
    "    df_cohort[phe_col_lst] = pd.get_dummies(df_phe, drop_first=True)\n",
    "\n",
    "    return df_cohort\n",
    "\n",
    "\n",
    "def create_matched_kfold_splits(df_map, df_cohort, num_splits):\n",
    "    df_case = df_cohort.sample(frac=1)\n",
    "    df_case = df_case.loc[df_case['CLASS'] == 1]\n",
    "\n",
    "    num_case_per_split = len(df_case) // num_splits\n",
    "\n",
    "    kfold_dict = {}\n",
    "\n",
    "    for i in range(num_splits):\n",
    "        fold_dict = {}\n",
    "\n",
    "        if i == (num_splits - 1):\n",
    "            df_case_split = df_case.iloc[i * num_case_per_split:]\n",
    "        else:\n",
    "            df_case_split = df_case.iloc[i * num_case_per_split:(i+1) * num_case_per_split]\n",
    "\n",
    "        print('split: {}\\t # of cases: {}'.format(i, len(df_case_split)))\n",
    "\n",
    "        df_matched_controls = df_case_split.merge(df_map,\n",
    "                                                  how='inner',\n",
    "                                                  on='GRID')\n",
    "\n",
    "        print('\\t\\t # of matched controls: {}'.format(len(df_matched_controls)))\n",
    "\n",
    "        test_case_control_lst = list(df_case_split['GRID'])\n",
    "\n",
    "        test_case_control_lst.extend(list(df_matched_controls['GRID_CONTROL']))\n",
    "\n",
    "        fold_dict['test'] = test_case_control_lst\n",
    "\n",
    "        print('\\t\\t # of unique cases and matched controls:\\t{}'.format(len(set(test_case_control_lst))))\n",
    "\n",
    "        train_case_control_lst = list(set(df_cohort['GRID']) - set(test_case_control_lst))\n",
    "\n",
    "        fold_dict['train'] = train_case_control_lst\n",
    "\n",
    "        kfold_dict[i] = fold_dict\n",
    "\n",
    "    return kfold_dict\n",
    "\n",
    "\n",
    "def std_scale_train_test(df_train, df_test):\n",
    "    print('BEGIN STD SCALE')\n",
    "\n",
    "    X_train = df_train.drop(['CLASS', 'GRID'], axis=1)\n",
    "    X_test = df_test.drop(['CLASS', 'GRID'], axis=1)\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    std_scaler.fit(X_train)\n",
    "\n",
    "    X_train_std = std_scaler.transform(X_train)\n",
    "    X_test_std = std_scaler.transform(X_test)\n",
    "\n",
    "    X_train_std = pd.DataFrame(X_train_std)\n",
    "    X_train_std.columns = list(X_train.columns)\n",
    "\n",
    "    X_test_std = pd.DataFrame(X_test_std)\n",
    "    X_test_std.columns = list(X_test.columns)\n",
    "\n",
    "    X_test_std['CLASS'] = list(df_test['CLASS'])\n",
    "    X_test_std['GRID'] = list(df_test['GRID'])\n",
    "\n",
    "    X_train_std['CLASS'] = list(df_train['CLASS'])\n",
    "    X_train_std['GRID'] = list(df_train['GRID'])\n",
    "\n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "\n",
    "def xgb_regularization(df_cohort, thresh):\n",
    "    df_features = df_cohort.drop(['GRID', 'CLASS'], axis=1)\n",
    "\n",
    "    clf_feat = SelectFromModel(estimator=XGBClassifier().fit(X=df_features,\n",
    "                                                             y=df_cohort['CLASS']),\n",
    "                               prefit=True, threshold=thresh)\n",
    "\n",
    "    clf_feat.transform(df_features)\n",
    "\n",
    "    feat_mask = list(clf_feat.get_support())\n",
    "    lst_select_feat = list()\n",
    "    for it in range(len(feat_mask)):\n",
    "        if feat_mask[it]:\n",
    "            lst_select_feat.append(list(df_features.columns)[it])\n",
    "\n",
    "    df_cohort_select_feat = df_cohort[['GRID', 'CLASS'] + lst_select_feat]\n",
    "\n",
    "    print('NUM FEATURES SELECTED: {}'.format(len(df_cohort_select_feat.columns)))\n",
    "\n",
    "    return df_cohort_select_feat\n",
    "\n",
    "\n",
    "# lab_lst = [col for col in list(df_cohort.columns) if '(' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if cmd_arg.impute_0:\n",
    "#     df_cohort = pd.read_csv(os.path.join(data_path, 'df_fram_cohort_0.csv'))\n",
    "# else:\n",
    "#     df_cohort = pd.read_csv(os.path.join(data_path, 'df_fram_cohort.csv'))\n",
    "# if cmd_arg.impute_0:\n",
    "#     df_cohort = pd.read_csv(os.path.join(data_path, 'df_full_cohort_0.csv'))\n",
    "#     df_cohort = df_cohort.drop(['predict'], axis=1)\n",
    "# else:\n",
    "#     df_cohort = pd.read_csv(os.path.join(data_path, 'df_full_cohort.csv'))\n",
    "#     df_cohort = df_cohort.drop(['predict'], axis=1)\n",
    "\n",
    "df_fram_cohort = pd.read_csv(os.path.join(data_path, 'df_fram_cohort.csv'))\n",
    "\n",
    "# df_fram_cohort = convert_phemed_to_bin(df_fram_cohort)\n",
    "\n",
    "df_fram_cohort = df_fram_cohort.drop(['AGE_AT_BASELINE', 'RACE_W', 'RACE_B', 'RACE_U', 'GENDER'], axis=1)\n",
    "\n",
    "    # df_cohort = l1_regularization(df=df_cohort,\n",
    "    #                               alpha_inv=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fram_cohort_reg = xgb_regularization(df_cohort=df_fram_cohort, thresh='10*mean')\n",
    "# df_fram_cohort_reg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_xgboost(df_train, df_test, params, fold):\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "\n",
    "    X_train, y_train = df_train.drop(['GRID', 'CLASS'], axis=1), df_train['CLASS']\n",
    "    X_test, y_test = df_test.drop(['GRID', 'CLASS'], axis=1), df_test['CLASS']\n",
    "\n",
    "    print(clf.get_params())\n",
    "\n",
    "    # print(X_train.head())\n",
    "\n",
    "    clf.fit(X=X_train,\n",
    "            y=y_train,\n",
    "#             eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "#             eval_metric='logloss',\n",
    "            early_stopping_rounds=8,\n",
    "            verbose=True)\n",
    "\n",
    "#     eval_results = clf.evals_result()\n",
    "    df_eval = pd.DataFrame({**(list(eval_results.values())[0]), **(list(eval_results.values())[1])})\n",
    "\n",
    "    df_pred = get_pred(clf=clf,\n",
    "                            df_test=df_test,\n",
    "                            fold=fold,\n",
    "                       model='xgb')\n",
    "\n",
    "    df_feat = pd.DataFrame({'FEATURE': list(X_train.columns), 'WEIGHT': list(clf.feature_importances_)})\n",
    "\n",
    "    return {'pred': df_pred, 'feat': df_feat, 'eval': df_eval}\n",
    "\n",
    "\n",
    "# def run_logistic_regression(df_test, df_train, params, fold):\n",
    "#     print('Run logistic regression')\n",
    "\n",
    "#     X_train, y_train = df_train.drop(['GRID', 'CLASS'], axis=1), df_train['CLASS']\n",
    "#     X_test, y_test = df_test.drop(['GRID', 'CLASS'], axis=1), df_test['CLASS']\n",
    "\n",
    "#     # print(X_train.head())\n",
    "\n",
    "#     clf = LogisticRegression(**params)\n",
    "\n",
    "#     print(clf.get_params())\n",
    "\n",
    "#     ts1 = time.time()\n",
    "#     clf.fit(X=X_train,\n",
    "#             y=y_train)\n",
    "#     ts2 = time.time()\n",
    "#     print('computation time:\\t{}'.format(ts2-ts1))\n",
    "\n",
    "#     df_pred = get_pred(clf=clf,\n",
    "#                        df_test=df_test,\n",
    "#                        fold=fold,\n",
    "#                        model='lr')\n",
    "\n",
    "#     df_feat = pd.DataFrame({'FEATURE': list(X_train.columns), 'WEIGHT': clf.coef_.T.flatten()})\n",
    "\n",
    "#     return {'pred': df_pred, 'feat': df_feat}\n",
    "\n",
    "\n",
    "# def run_random_forest(df_train, df_test, params, fold):\n",
    "#     print('run random forest')\n",
    "#     X_train, y_train = df_train.drop(['GRID', 'CLASS'], axis=1), df_train['CLASS']\n",
    "#     X_test, y_test = df_test.drop(['GRID', 'CLASS'], axis=1), df_test['CLASS']\n",
    "\n",
    "#     # print(X_train.head())\n",
    "\n",
    "#     clf = RandomForestClassifier(**params)\n",
    "\n",
    "#     print(clf.get_params)\n",
    "\n",
    "#     ts1 = time.time()\n",
    "#     clf.fit(X=X_train,\n",
    "#             y=y_train)\n",
    "#     ts2 = time.time()\n",
    "#     print('computation time:\\t{}'.format(ts2-ts1))\n",
    "\n",
    "#     df_pred = get_pred(clf=clf,\n",
    "#                        df_test=df_test,\n",
    "#                        fold=fold,\n",
    "#                        model='rf')\n",
    "\n",
    "#     # print(clf.feature_importances_.shape)\n",
    "#     df_feat = pd.DataFrame({'FEATURE': list(X_train.columns), 'WEIGHT': clf.feature_importances_})\n",
    "\n",
    "#     return {'pred': df_pred, 'feat': df_feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    params_lr = {'class_weight': 'balanced',\n",
    "                 'solver': 'sag',\n",
    "                 'verbose': 1, 'random_state': 1,\n",
    "                 'n_jobs': 1,\n",
    "                 'C': 1}\n",
    "\n",
    "    params_rf = {'n_estimators': 400,\n",
    "                 'max_features': .1,\n",
    "                 'max_depth': 8,\n",
    "                 'bootstrap': True,\n",
    "                 'verbose': 1,\n",
    "                 'class_weight': 'balanced',\n",
    "                 'min_samples_leaf': 1,\n",
    "                 'random_state': 1}\n",
    "\n",
    "    params_xgb = {'max_depth': 8, 'learning_rate': 0.1,\n",
    "                  'n_estimators': 400, 'silent': False,\n",
    "                  'objective': 'binary:logistic',\n",
    "                  'booster': 'gbtree', 'n_jobs': 4,\n",
    "                  'subsample': 1.0, 'scale_pos_weight': 10, 'random_state': 1}\n",
    "    \n",
    "    params_gbt = {'verbose': 1, 'max_depth': 8}\n",
    "\n",
    "#     if not cmd_arg.cw:\n",
    "#         params_lr.pop('class_weight', 0)\n",
    "#         params_rf.pop('class_weight', 0)\n",
    "#         params_xgb.pop('scale_pos_weight', 0)\n",
    "\n",
    "    model_params = {'lr': params_lr, 'rf': params_rf, 'xgb': params_xgb, 'gbt': params_gbt}\n",
    "\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(clf, df_test, fold, model):\n",
    "    X_test = df_test.drop(['GRID', 'CLASS'], axis=1)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    df_pred = pd.DataFrame({'GRID': list(df_test['GRID']),\n",
    "                            'PRED': list(y_pred)})\n",
    "    df_pred['PROB(0)'] = y_prob[:, 0]\n",
    "    df_pred['PROB(1)'] = y_prob[:, 1]\n",
    "\n",
    "    df_pred['CLASS'] = list(df_test['CLASS'])\n",
    "\n",
    "    if model == 'lr' or 'gbt':\n",
    "        df_pred['SCORE'] = clf.decision_function(X_test)\n",
    "    else:\n",
    "        df_pred['SCORE'] = df_pred['PROB(1)']\n",
    "\n",
    "    df_pred['FOLD'] = [fold] * len(df_pred)\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of subjects in cohort:\t93348\n",
      "# of features in cohort:\t2634\n",
      "split: 0\t # of cases: 1037\n",
      "\t\t # of matched controls: 8296\n",
      "\t\t # of unique cases and matched controls:\t9333\n",
      "split: 1\t # of cases: 1037\n",
      "\t\t # of matched controls: 8296\n",
      "\t\t # of unique cases and matched controls:\t9333\n",
      "split: 2\t # of cases: 1037\n",
      "\t\t # of matched controls: 8296\n",
      "\t\t # of unique cases and matched controls:\t9333\n",
      "split: 3\t # of cases: 1037\n",
      "\t\t # of matched controls: 8296\n",
      "\t\t # of unique cases and matched controls:\t9333\n",
      "split: 4\t # of cases: 1037\n",
      "\t\t # of matched controls: 8296\n",
      "\t\t # of unique cases and matched controls:\t9333\n",
      "split: 5\t # of cases: 1037\n",
      "\t\t # of matched controls: 8296\n",
      "\t\t # of unique cases and matched controls:\t9333\n",
      "split: 6\t # of cases: 1037\n",
      "\t\t # of matched controls: 8296\n",
      "\t\t # of unique cases and matched controls:\t9333\n",
      "split: 7\t # of cases: 1037\n",
      "\t\t # of matched controls: 8296\n",
      "\t\t # of unique cases and matched controls:\t9333\n",
      "split: 8\t # of cases: 1037\n",
      "\t\t # of matched controls: 8296\n",
      "\t\t # of unique cases and matched controls:\t9333\n",
      "split: 9\t # of cases: 1039\n",
      "\t\t # of matched controls: 8312\n",
      "\t\t # of unique cases and matched controls:\t9351\n",
      "FOLD: 0\n",
      "\t# of train subject: 84015\n",
      "\t# of test subject: 9333\n",
      "BEGIN STD SCALE\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 8, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'presort': 'auto', 'random_state': None, 'subsample': 1.0, 'verbose': 1, 'warm_start': False}\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6781          123.76m\n",
      "         2           0.6651          124.39m\n",
      "         3           0.6547          127.52m\n",
      "         4           0.6465          125.13m\n",
      "         5           0.6389          125.28m\n",
      "         6           0.6313          130.95m\n",
      "         7           0.6254          134.13m\n",
      "         8           0.6205          134.56m\n",
      "         9           0.6152          135.71m\n",
      "        10           0.6103          133.31m\n",
      "        20           0.5753          112.37m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_folds=10\n",
    "\n",
    "df_cohort = df_fram_cohort\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds)\n",
    "skf.get_n_splits(X=df_cohort,\n",
    "                 y=df_cohort['CLASS'])\n",
    "\n",
    "feat_lst = list(df_cohort.columns)\n",
    "feat_lst.remove('CLASS')\n",
    "feat_lst.remove('GRID')\n",
    "\n",
    "# df_pred_cv = pd.DataFrame()\n",
    "# df_feat_cv = pd.DataFrame({'FEATURE': feat_lst})\n",
    "\n",
    "print('# of subjects in cohort:\\t{}'.format(len(df_cohort)))\n",
    "print('# of features in cohort:\\t{}'.format(len(df_cohort.columns)))\n",
    "\n",
    "# fold = 0\n",
    "# for train_index, test_index in skf.split(X=df_cohort,\n",
    "#                                          y=df_cohort['CLASS']):\n",
    "kfold_dict = create_matched_kfold_splits(df_map=pd.read_csv(os.path.join('/legodata/zhaoj/cvd_risk_time2/data_1_8', 'raw', 'control_case_mappings.csv')),\n",
    "                                         df_cohort=df_cohort,\n",
    "                                         num_splits=n_folds)\n",
    "\n",
    "fold = 0 \n",
    "\n",
    "params = get_params()\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    train_index = (kfold_dict[fold])['train']\n",
    "    test_index = (kfold_dict[fold])['test']\n",
    "\n",
    "    print('FOLD: {}'.format(fold))\n",
    "\n",
    "    # df_train, df_test = df_cohort.iloc[train_index], df_cohort.iloc[test_index]\n",
    "    df_train, df_test = df_cohort.loc[df_cohort['GRID'].isin(train_index)], df_cohort.loc[df_cohort['GRID'].isin(test_index)]\n",
    "\n",
    "    print('\\t# of train subject: {}'.format(len(df_train)))\n",
    "    print('\\t# of test subject: {}'.format(len(df_test)))\n",
    "\n",
    "    df_train, df_test = std_scale_train_test(df_train=df_train,\n",
    "                                             df_test=df_test)\n",
    "\n",
    "#         clf_dict = run_logistic_regression(df_train=df_train,\n",
    "#                                            df_test=df_test,\n",
    "#                                            params=params['lr'],\n",
    "#                                            fold=fold)\n",
    "#         clf_dict = run_random_forest(df_train=df_train,\n",
    "#                                      df_test=df_test,\n",
    "#                                      params=params['rf'],\n",
    "#                                      fold=fold)\n",
    "\n",
    "    clf = GradientBoostingClassifier(**(params['gbt']))\n",
    "    \n",
    "\n",
    "    X_train, y_train = df_train.drop(['GRID', 'CLASS'], axis=1), df_train['CLASS']\n",
    "    X_test, y_test = df_test.drop(['GRID', 'CLASS'], axis=1), df_test['CLASS']\n",
    "\n",
    "    print(clf.get_params())\n",
    "\n",
    "    # print(X_train.head())\n",
    "\n",
    "    clf.fit(X=X_train,\n",
    "            y=y_train) \n",
    "\n",
    "#     eval_results = clf.evals_result()\n",
    "#     df_eval = pd.DataFrame({**(list(eval_results.values())[0]), **(list(eval_results.values())[1])})\n",
    "\n",
    "    df_pred = get_pred(clf=clf,\n",
    "                            df_test=df_test,\n",
    "                            fold=fold,\n",
    "                       model='gbt')\n",
    "\n",
    "#     df_feat = pd.DataFrame({'FEATURE': list(X_train.columns), 'WEIGHT': list(clf.feature_importances_)})\n",
    "\n",
    "    # if model == 'rf':\n",
    "    #     print(clf_dict['pred'])\n",
    "\n",
    "#     df_pred_cv = pd.concat([df_pred_cv, clf_dict['pred']])\n",
    "#     df_feat_rank = clf_dict['feat']\n",
    "#     df_feat_rank = df_feat_rank.rename(columns={'WEIGHT': 'WEIGHT{}'.format(fold)})\n",
    "    # needs to be inner so that feature reduction will allow # of features in df_feat_cv to be reduced\n",
    "#     df_feat_cv = df_feat_cv.merge(df_feat_rank, on='FEATURE', how='inner')\n",
    "\n",
    "    # print(df_feat_rank.sort_values(by='WEIGHT{}'.format(fold), ascending=False).head())\n",
    "    break \n",
    "\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame({'feature': feat_lst, 'weight': list(clf.feature_importances_)})\n",
    "df_feat.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}